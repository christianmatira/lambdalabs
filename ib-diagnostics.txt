IB Diagnostics - layer 1 - where a card is stuck in Polling

Add:
   hca_self_test.ofed
   perfquery

General setup: (so check these)
 1. Mellanox will generally ask for you to upgrade your switch and your card to the latest firmware
 2. Should not enable computers 'fast boot'
 3. Checking BIOS options:
    On SMC it works by default:
    a. Get the BIOS settings
       export BMC_IP=$(sudo ipmitool lan print 1 | grep "IP Address" | egrep -v Source | awk '{print $4}')
       curl -s -k -H'Content-type: application/json' -X GET https:/${BMC_IP}/registries/BiosAttributeRegistry.1.0.0.json > $(uname -n)-bios.json
    b. Search the bios.json for settings: 
       (This is for Supermicro needs verification on each chassis types, based on the BIOS options:
           4124GO-NART+: ACS, IOMMU, SMEE, SMT, Determinism Control  (NOTE: we did not change TSME)
           4124GS-TNR: ACS, IOMMU, SMT, Determinism Control  (NOTE: No SMEE, need to confirm at TSME 
                        - I would leave or disable TSME as it adds latency 5 nsâ€“7 ns of additional memory latency.
                        - Source of timing: The BIOS info.
      (SME vs TSME: https://mricher.fr/post/amd-memory-encryption/)
       (json from above)
       cat $(uname -n)-bios.json | jq | egrep -B 4 -A 4 "ACS|IOMMU|SMEE|SMT|Determinism Control"

       Also these are what we are currently using:
         ACS -disabled      
         IOMMU - disabled
         SMEE - enable - secure memory encryption (on 4124GO-NART+)
         SMT - disabled - so real threads (one thread per core).
         Determinism Control - disabled - turned off Power Savings

       Also confirm 'fast boot' is off - desktops
       For SMT - there are other options and leave multithreading depending on the code it may perform better with threading.
                 (normally for codes that have dangling threads).

 3. Check CPU frequency settings:
    a. Disable the ondemand: systemctl disable ondemand and set the CPUs to performance (versus ondemand)
       NOTE: Available options are in:
             cat /sys/devices/system/cpu/cpu1/cpufreq/scaling_available_governors

       $ sudo systemctl disable ondemand
       $ cat << EOF >> cpu_freq.sh
       > #!/bin/sh
       > 
       > GOVERNOR=performance
       > for CPUFREQ in /sys/devices/system/cpu/cpu*/cpufreq/scaling_governor
       > do
       >         [ -f $CPUFREQ ] || continue
       >         echo -n $GOVERNOR > $CPUFREQ
       > done
       > EOF
    b. Run the cpu_freq.sh
       $ sudo sh ./cpu_freq.sh

4. Check the Mellanox driver version
       $ ibstat | grep "Firmware version:" | sort | uniq -c 
          8 	Firmware version: 20.34.1002

This can be caused by various issues:
 1. Cable - common:
     * Ideally: swap both ends with a working cable
        - If failure follows the cable - replace cable
        - If it is now working - Likely reseat was the issue
        - If the problem remains on the same node
           * swap one end to confirm the card is the issue versus the switch port
           * If the card, reflash the card (if not previously done)
           * Reseat or replace card
 2. Cable Transciever 
        - ProLabs - cables - update the FirmWare (on-site)
 3. Firmware - reflashing the card
 4. Card - see #1 for debugging, but at times a card failure
 4. Switch port - see #1 for debugging - solution: use a alternate port label that port
 5. OpenSM running is sometimes a issue - commonly run on the switch (or up to three nodes)

General data collection: (since ibqueryerrors does not always work if the machine is in a bad state): 
  $ export DIRNAME=diags-$(uname -n)
  $ mkdir ${DIRNAME}
  $ cd ${DIRNAME}
  $ ibstat >& ibstat.txt
  $ sudo sminfo >& sminfo.txt
  $ sudo ibdiagnet -o ./ibdiagnet2 -pc --pm_pause_time 60 2>&1 | tee -a ibdiag-net.txt
  $ sudo ibqueryerrors 2>&1 | tee -a ibqueryerrors.txt
  $ sudo dmesg > dmesg.txt
  $ sudo journalctl > journalctl.txt
  $ sudo cp /var/log/kern.log* .
  $ cd ..
  $ sudo tar -zcf ${DIRNAME}.tgz ${DIRNAME}

Other useful commands are:
       ibnodes
       ibhosts
       ibswitches
       iblinkinfo
       ibnetdiscover
       ibv_devinfo
       ibping - Two side server/client connection check

For ibqueryerrors, you are watching for the 'change in the number' of errors,
If a cable was reseated or node rebooted, it will see errors.
The 'ibdiagnet -pc' clears previous error counters and in the command above then looks at errors.
   * Which is why I run ibqueryerrors after or I run ibqueryerrors for monitoring
 
The most interesting errors are: (stole this list from https://github.com/guilbaults/infiniband-exporter/issues/12)
 1. SymbolErrorCounter -> Problem with a cable
 2. PortXmitDiscards -> Too much congestion and lossless feature of IB was not working, some packets were dropped
 3. LinkDownedCounter -> Can happen with a bad cable that flap between up/down
 4. PortRcvRemotePhysicalErrors -> Problem with a cable

I mostly watch for SymbolErrorCounter and LinkDownCounter.
 
We have seen cards/firmware get in 'bad states', but that should be infrequent.
